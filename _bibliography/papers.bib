---
---

@inproceedings{OSDI24:BurstCBS,
author = {Junyi Shu and Kun Qian and Ennan Zhai and Xuanzhe Liu and Xin Jin},
title = {Burstable Cloud Block Storage with Data Processing Units},
year = {2024},
isbn = {978-1-939133-40-3},
publisher = {USENIX Association},
booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
address = {Santa Clara, CA},
pages = {783--799},
url = {https://www.usenix.org/conference/osdi24/presentation/shu},
publisher = {USENIX Association},
month = jul,
abstract = {Cloud block storage (CBS) is a key pillar of public clouds. Today's CBS distinguishes itself from physical counterparts (e.g., SSDs) by offering unique burst capability as well as enhanced throughput, capacity, and availability. We conduct an initial characterization of our CBS product, a globally deployed cloud block storage service at public cloud provider Alibaba Cloud. A key observation is that the storage agent (SA) running on a data processing unit (DPU) which connects user VMs to the backend storage is the major source of performance fluctuation with burst capability provided. In this paper, we propose a hardware-software co-designed I/O scheduling system BurstCBS to address load imbalance and tenant interference at SA. BurstCBS exploits high-performance queue scaling to achieve near-perfect load balancing at line rate. To mitigate tenant interference, we design a novel burstable I/O scheduler that prioritizes resource allocation for base-level usage while supporting bursts. We employ a vectorized I/O cost estimator for comprehensive measurements of the consumed resources of different types of I/Os. Our evaluation shows that BurstCBS reduces average latency by up to 85% and provides up to 5× throughput for base-level tenants under congestion with minimal overhead. We verify the benefits brought by BurstCBS with a database service that internally relies on CBS, and show that up to 83% latency reduction is observed on customer workloads.},
selected={true},
abbr={OSDI},
pdf={osdi24_BurstCBS_paper.pdf},
slides={osdi24_BurstCBS_slides.pptx}
}

@inproceedings{ASPLOS23:dRAID,
author = {Junyi Shu and Ruidong Zhu and Yun Ma and Gang Huang and Hong Mei and Xuanzhe Liu and Xin Jin},
title = {Disaggregated RAID Storage in Modern Datacenters},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582027},
doi = {10.1145/3582016.3582027},
abstract = {RAID (Redundant Array of Independent Disks) has been widely adopted for decades, as it provides enhanced throughput and redundancy beyond what a single disk can offer. Today, enabled by fast datacenter networks, accessing remote block devices with acceptable overhead (i.e. disaggregated storage) becomes a reality (e.g., for serverless applications). Combining RAID with remote storage can provide the same benefits while creating better fault tolerance and flexibility than its monolithic counterparts. The key challenge of disaggregated RAID is to handle extra network traffic generated by RAID, which can consume a vast amount of NIC bandwidth. We present dRAID, a disaggregated RAID system that achieves near-optimal read and write throughput. dRAID exploits peer-to-peer disaggregated data access to reduce bandwidth consumption in both normal and degraded states. It employs non-blocking multi-stage writes to maximize inter-node parallelism, and applies pipelined I/O processing to maximize inter-device parallelism. We introduce bandwidth-aware reconstruction for better load balancing. We show that dRAID provides up to 3x bandwidth improvement. The results on a lightweight object store show that dRAID brings 1.5x-2.35x throughput improvement on various workloads.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {147–163},
numpages = {17},
keywords = {RDMA, RAID, NVMe-oF, Disaggregated Storage},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023},
selected={true},
abbr={ASPLOS},
pdf={asplos23_dRAID_paper.pdf},
slides={asplos23_dRAID_slides.pptx},
code={https://github.com/pkusys/dRAID},
poster={asplos23_dRAID_poster.pdf}
}

@inproceedings{SIGCOMM21Poster:multi_region,
author = {Junyi Shu and Xin Jin and Yun Ma and Xuanzhe Liu and Gang Huang},
title = {Cost-Effective Data Analytics across Multiple Cloud Regions},
year = {2021},
isbn = {9781450386296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472716.3472842},
doi = {10.1145/3472716.3472842},
abstract = {We propose a cloud-native data analytics engine for processing data stored among geographically
distributed cloud regions with reduced cost. A job is split into subtasks and placed
across regions based on factors including prices of compute resources and data transmission.
We present its architecture which leverages existing cloud infrastructures and discuss
major challenges of its system design. Preliminary experiments show that the cost
is reduced by 15.1% for a decision support query on a four-region public cloud setup.},
booktitle = {Proceedings of the SIGCOMM '21 Poster and Demo Sessions},
pages = {1–3},
numpages = {3},
keywords = {data analytics, job scheduling, multi-cloud, cost optimization},
location = {Virtual Event},
series = {SIGCOMM '21},
selected={false},
abbr={SIGCOMM},
pdf={sigcomm21_multi_region_paper.pdf},
poster={sigcomm21_multi_region_poster.pdf}
}